{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":6300093,"sourceType":"datasetVersion","datasetId":3623988},{"sourceId":6857742,"sourceType":"datasetVersion","datasetId":3623154},{"sourceId":6865136,"sourceType":"datasetVersion","datasetId":3945154},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":6987454,"sourceType":"datasetVersion","datasetId":3947266},{"sourceId":7309211,"sourceType":"datasetVersion","datasetId":4241173},{"sourceId":7351121,"sourceType":"datasetVersion","datasetId":4269057},{"sourceId":7397564,"sourceType":"datasetVersion","datasetId":4301221},{"sourceId":7413041,"sourceType":"datasetVersion","datasetId":4295405},{"sourceId":7414139,"sourceType":"datasetVersion","datasetId":4312878},{"sourceId":7414267,"sourceType":"datasetVersion","datasetId":4312975},{"sourceId":7439109,"sourceType":"datasetVersion","datasetId":4329654},{"sourceId":7439427,"sourceType":"datasetVersion","datasetId":4329880},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing library","metadata":{}},{"cell_type":"code","source":"!pip install -q -U accelerate --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T04:22:18.857849Z","iopub.execute_input":"2024-01-31T04:22:18.858206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/input/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl --no-deps\n!pip install /kaggle/input/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl --no-deps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport gc\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time \nimport os\nimport itertools\nimport regex as re\nfrom tqdm import tqdm\n\n\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom scipy.sparse import csr_matrix, vstack, hstack\n\nfrom concurrent.futures import ProcessPoolExecutor\n\n\n# Setting hyperparameters\nseed = 202 # set the seed for reproducibility  2023->202\nisFixTestLeakage = True # TODO!!! reference: https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/455701\nisCorrectSentence = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"torch\"\nimport keras_nlp\nimport keras_core as keras \nimport keras_core.backend as K\nimport jax\nimport tensorflow as tf\nfrom glob import glob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seed Everything & Correct Sentence","metadata":{}},{"cell_type":"code","source":"import random\ndef seed_everything(seed=2023):\n    random.seed(seed)\n    np.random.seed(seed)\n\nseed_everything(seed)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing files and Feature Engineering","metadata":{}},{"cell_type":"code","source":"seed_everything()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport pickle\nimport torch\n\nFAKE_SUBMISSION = True\n\n\ntest_essays_csv_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n\n\ndaigt2_val = pd.read_csv(\"/kaggle/input/daigt2-wo-official-with-prompt-id/validation.csv\")\ndaigt2_val.reset_index(drop=True, inplace=True)\n\n\n\nIS_LIVE_SUBMISSION = True\nFOLLOWER_MODEL_WEIGHT = 1.1\n\nmistral_heuristic_cache = None\nertugrul_deberta_cache = None\n\nif IS_LIVE_SUBMISSION:\n    daigt2_train = pd.read_csv(\"/kaggle/input/typo-intro/typo_intro.csv\", sep=',')\n    daigt2_train = daigt2_train.drop_duplicates(subset=['text'])\n    daigt2_train.reset_index(drop=True, inplace=True)\n    test = test_essays_csv_df\n    MAX_EXAMPLES_TO_PREDICT = 1000000\n    GBOOST_ITERATION_FRACTION = 1\n    SVC_BAGGING_FRACTION = .3\n\nelse:    \n    if FAKE_SUBMISSION:\n        if not torch.cuda.is_available(): raise RuntimeError(\"CUDA is not available.\")\n        sub = pd.DataFrame({'id':[],'generated':[]})\n        sub.to_csv('submission.csv', index=False)\n        sys.exit()\n        \n    daigt2_train = pd.read_csv(\"/kaggle/input/typo-intro/typo_intro.csv\", sep=',')\n    daigt2_train.reset_index(drop=True, inplace=True)\n\n    mistral_heuristic_cache = {}\n    with open(\"/kaggle/input/jan13-quantile-string-mistral-cache/jan_13_test_quantiles.pkl\", 'rb') as quantiles_pickle_file:\n        mistral_heuristic_cache[\"test\"] = pickle.load(quantiles_pickle_file)\n        \n    with open(\"/kaggle/input/erturgrul-deberta-preds-cache-jan16/ertugrul_deberta_predictions.pkl\", 'rb') as deberta_preds_file:\n        ertugrul_deberta_cache = pickle.load(deberta_preds_file)\n\n    test = pd.read_csv(\"/kaggle/input/daigt2-wo-official-with-prompt-id/test_no_label.csv\")\n    MAX_EXAMPLES_TO_PREDICT = 1200\n    GBOOST_ITERATION_FRACTION = .1\n    SVC_BAGGING_FRACTION = .1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excluded_prompt_name_list = ['Distance learning','Grades for extracurricular activities','Summer projects']\ndaigt2_train = daigt2_train[~(daigt2_train['prompt_name'].isin(excluded_prompt_name_list))]\n\ndaigt2_train = daigt2_train.drop_duplicates(subset=['text'])\ndaigt2_train.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def safe_logodds(prob_arr):\n    clipped = np.clip(a=prob_arr, a_min=.001, a_max=.999)\n    return np.log(clipped/(1-clipped))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import DataLoader\n\ntorch.cuda.empty_cache()\n\n\nif ertugrul_deberta_cache is None:\n\n    @dataclass\n    class ertugrul_debert_cfg:\n        transformer_name = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-large\"\n        batch_size = 8\n        max_len = 1024\n        n_classes = 1\n\n\n    ertugrul_debert_tokenizer = AutoTokenizer.from_pretrained(ertugrul_debert_cfg.transformer_name)\n\n\n    def ertugrul_deberta_prepare_input(text):\n        inputs = ertugrul_debert_tokenizer.encode_plus(\n            text, \n            return_tensors=None, \n            add_special_tokens=True, \n            max_length=ertugrul_debert_cfg.max_len,\n            pad_to_max_length=True,\n            truncation=True,\n            return_token_type_ids=True\n        )\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n        return inputs\n\n\n    class ErtugrulDebertaTrainDataset(Dataset):\n        def __init__(self, df):\n            self.texts = df['text'].values\n\n        def __len__(self):\n            return len(self.texts)\n\n        def __getitem__(self, item):\n            inputs = ertugrul_deberta_prepare_input(self.texts[item])\n            return inputs\n\n\n    def ertugrul_deberta_collate(inputs):\n        mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n        for k, v in inputs.items():\n            inputs[k] = inputs[k][:,:mask_len]\n        return inputs\n\n\n    ertugrul_deberta_test_dataset = ErtugrulDebertaTrainDataset(test)\n\n\n    ertugrul_deberta_checkpoint = torch.load(\"/kaggle/input/deblarge-f0-999/last.ckpt\")\n\n    class ClassicFeed(torch.nn.Module):\n        def __init__(self):\n            super(ClassicFeed, self).__init__()\n\n            self.transformer = AutoModel.from_pretrained(ertugrul_debert_cfg.transformer_name)\n            self.classifier = torch.nn.Linear(self.transformer.config.hidden_size, 1)\n\n        def forward(self, input_ids, attention_mask, token_type_ids):\n            output = self.transformer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            last_layer_hidden_states = output.last_hidden_state[:, 0, :]\n            outputs = self.classifier(last_layer_hidden_states)\n            return outputs\n\n    classic_model = ClassicFeed()\n    classic_model.load_state_dict(ertugrul_deberta_checkpoint['state_dict'])\n\n    classic_model.eval()\n    trained_model = classic_model.to('cuda')\n    ertugrul_deberta_dataloader = DataLoader(ertugrul_deberta_test_dataset, batch_size=ertugrul_debert_cfg.batch_size, shuffle=False)\n\n    ertugrul_deberta_predictions = []\n\n    for batch in tqdm(ertugrul_deberta_dataloader, position=0, leave=True):\n\n        batch = ertugrul_deberta_collate(batch)  \n        for k, v in batch.items():\n            batch[k] = v\n\n\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        token_type_ids = batch['token_type_ids'].to('cuda')\n\n        # Wrap the prediction in torch.no_grad()\n        with torch.no_grad():\n            deberta_pred = trained_model(input_ids, attention_mask, token_type_ids)\n\n        ertugrul_deberta_predictions.append(torch.sigmoid(deberta_pred))\n        del batch\n\n    ertugrul_deberta_predictions = torch.cat(ertugrul_deberta_predictions).cpu().numpy()[:,0]\n\n    del trained_model\n    del classic_model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nelse:\n    ertugrul_deberta_predictions = ertugrul_deberta_cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nchunk_model = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/input/longformer-predict-model-best-check\",\n)\n\nchunk_tokenizer = AutoTokenizer.from_pretrained(\n    \"/kaggle/input/longformer-predict-model-best-check\"\n)\n\n\n\nchunk_model.to(\"cuda\")\n\nprint(\"starting\")\npredict_model_features = []\nfor test_text in tqdm(test[\"text\"].values):\n\n    tokenized_test_text = chunk_tokenizer(test_text, return_tensors=\"pt\")\n\n\n    input_ids = tokenized_test_text['input_ids'].to('cuda')\n    attention_mask = tokenized_test_text['attention_mask'].to('cuda')\n\n    # Wrap the prediction in torch.no_grad()\n    with torch.no_grad():\n        chunk_pred = safe_logodds(torch.softmax(chunk_model(input_ids, attention_mask).logits[0], dim=-1).cpu().numpy())\n        predict_model_features.append(chunk_pred)\n\n    del chunk_pred\n    \n\n\ndel chunk_model\ngc.collect()\ntorch.cuda.empty_cache()\n\npredict_model_features = np.array(predict_model_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport transformers\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = None\nif mistral_heuristic_cache is None:\n    model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            load_in_4bit=True,\n            quantization_config=bnb_config,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            trust_remote_code=True,\n        )\n\n    model.config.pretraining_tp = 1 \n    model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @misc{bao2023fast,\n#     url = {https://arxiv.org/abs/2310.05130},\n#     author = {Bao, Guangsheng and Zhao, Yanbin and Teng, Zhiyang and Yang, Linyi and Zhang, Yue},\n#     title = {Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature},\n#     publisher = {arXiv},\n#     year = {2023},\n# }\n# All code in this cell was originally copied from https://github.com/baoguangsheng/fast-detect-gpt .\n# I then modified some of it for our purposes.\ndef get_samples(logits, labels):\n    assert logits.shape[0] == 1\n    assert labels.shape[0] == 1\n    nsamples = 10000\n    lprobs = torch.log_softmax(logits, dim=-1)\n    distrib = torch.distributions.categorical.Categorical(logits=lprobs)\n    samples = distrib.sample([nsamples]).permute([1, 2, 0])\n    return samples\n\ndef get_likelihood(logits, labels):\n    assert logits.shape[0] == 1\n    assert labels.shape[0] == 1\n    labels = labels.unsqueeze(-1) if labels.ndim == logits.ndim - 1 else labels\n    lprobs = torch.log_softmax(logits, dim=-1)\n    log_likelihood = lprobs.gather(dim=-1, index=labels)\n    return log_likelihood.mean(dim=1)\n\ndef get_sampling_discrepancy(logits_ref, logits_score, labels):\n    assert logits_ref.shape[0] == 1\n    assert logits_score.shape[0] == 1\n    assert labels.shape[0] == 1\n    assert logits_ref.shape[1] == labels.shape[1]\n    if logits_ref.size(-1) != logits_score.size(-1):\n        # print(f\"WARNING: vocabulary size mismatch {logits_ref.size(-1)} vs {logits_score.size(-1)}.\")\n        vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n        logits_ref = logits_ref[:, :, :vocab_size]\n        logits_score = logits_score[:, :, :vocab_size]\n\n    samples = get_samples(logits_ref, labels)\n    log_likelihood_x = get_likelihood(logits_score, labels)\n    log_likelihood_x_tilde = get_likelihood(logits_score, samples)\n    miu_tilde = log_likelihood_x_tilde.mean(dim=-1)\n    sigma_tilde = log_likelihood_x_tilde.std(dim=-1)\n    discrepancy = (log_likelihood_x.squeeze(-1) - miu_tilde) / sigma_tilde\n    return discrepancy.item(), log_likelihood_x.squeeze(-1), miu_tilde, sigma_tilde\n\ndef get_sampling_discrepancy_analytic(logits_ref, logits_score, labels):\n    assert logits_ref.shape[0] == 1\n    assert logits_score.shape[0] == 1\n    assert labels.shape[0] == 1\n    assert logits_ref.shape[1] == labels.shape[1]\n    if logits_ref.size(-1) != logits_score.size(-1):\n        raise Exception()\n\n    assert labels.ndim == logits_score.ndim - 1\n    labels = labels.unsqueeze(-1)\n    lprobs_score = torch.log_softmax(logits_score, dim=-1)\n    probs_ref = torch.softmax(logits_ref, dim=-1)\n    log_likelihood = lprobs_score.gather(dim=-1, index=labels).squeeze(-1)\n    mean_ref = (probs_ref * lprobs_score).sum(dim=-1)\n    var_ref = (probs_ref * torch.square(lprobs_score)).sum(dim=-1) - torch.square(mean_ref)\n    discrepancy = (log_likelihood.sum(dim=-1) - mean_ref.sum(dim=-1)) / var_ref.sum(dim=-1).sqrt()\n    discrepancy = discrepancy.mean()\n    return discrepancy.item(), mean_ref.mean(dim=-1).item(), (var_ref.sum(dim=-1).sqrt() / mean_ref.shape[-1]).item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.stats\nclass ImpliedTempScorer():\n\n    def __init__(self, device=\"cuda\"):\n        self.norm = scipy.stats.norm()\n        self.num_samples = 60\n        self.batch_size = 20\n        self.fake_normal_samples = torch.tensor(self.norm.ppf(np.linspace(1/(self.num_samples+1), 1-1/(self.num_samples + 1), self.num_samples)))\n        self.fake_temp_samples = torch.exp(self.fake_normal_samples).to(torch.float16).to(device)\n        self.device = device\n\n    def get_ev_of_temp(self, logits_batch, indices):\n        # logits_batch should be a tensor of shape (batch_size, num_logits)\n        # indices should be a tensor of shape (batch_size)\n        \n        logits_batch = logits_batch.to(self.device).to(torch.float16)\n        indices = indices.to(self.device)\n\n        curr_subbatch_start = 0\n        all_final_temp_scores = []\n        while curr_subbatch_start < logits_batch.shape[0]:\n\n            curr_subbatch_end = min(curr_subbatch_start + self.batch_size, logits_batch.shape[0])\n            curr_subbatch = logits_batch[curr_subbatch_start:curr_subbatch_end]\n            curr_indices = indices[curr_subbatch_start:curr_subbatch_end]\n                \n\n            # shape shold be (batch_size, num_logits, num_temps)\n            all_logit_sets = torch.matmul(curr_subbatch.unsqueeze(-1), (1/self.fake_temp_samples).unsqueeze(0))\n            assert all_logit_sets.shape == (curr_subbatch.shape[0], logits_batch.shape[1], self.fake_temp_samples.shape[0])\n\n            probs = torch.exp(torch.nn.functional.log_softmax(all_logit_sets, dim=1))\n            prob_of_logit = probs[torch.arange(curr_subbatch.shape[0]), curr_indices]\n            assert prob_of_logit.shape == (curr_subbatch.shape[0], self.fake_temp_samples.shape[0])\n\n            prob_of_temp = prob_of_logit / prob_of_logit.sum(dim=-1, keepdim=True)\n            temp_evs = torch.matmul(prob_of_temp, self.fake_temp_samples).cpu().numpy()\n            \n            cdf_scores = self.norm.cdf(np.log(np.clip(temp_evs, 1e-10, None)))\n            right_cdf_scores = 1 - cdf_scores\n            log_right_cdf_scores = np.log(np.clip(right_cdf_scores, 1e-10, None))\n            all_final_temp_scores.extend(log_right_cdf_scores)\n\n            curr_subbatch_start = curr_subbatch_end\n\n        return np.array(all_final_temp_scores)\n    \nimplied_scorer = ImpliedTempScorer(device=\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n\n\ndef batched(iterable, n):\n    # batched('ABCDEFG', 3) --> ABC DEF G\n    if n < 1:\n        raise ValueError('n must be at least one')\n    it = iter(iterable)\n    while batch := tuple(itertools.islice(it, n)):\n        yield batch\n\ndef get_heuristics(input_df):\n    generated_perplexity = []\n    human_perplexity = []\n    all_perplexities = []\n    generated_discs = []\n    human_discs = []\n    generated_logprobs = []\n    human_logprobs = []\n    id_to_heuristics = {}\n\n    input_df = input_df.sort_values(by='text', key=lambda x: x.str.len(), ascending=False)\n    \n    np.random.seed(0)\n    unique_random_token_indices = np.random.choice(32000, 2000, replace=False)\n    \n\n    \n    for train_rows in tqdm(batched(input_df.iterrows(), n=10)):\n\n        train_rows = [train_row[1] for train_row in train_rows]\n        texts = [train_row[\"text\"] for train_row in train_rows]\n        example_ids = [train_row[\"id\"] for train_row in train_rows]\n\n        tokenized = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=800)\n        input_ids = tokenized.input_ids\n        mask = tokenized.attention_mask\n        with torch.no_grad():\n            opt = model(**tokenized)\n            \n        del tokenized\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        logits = opt.logits\n        for batch_idx in range(input_ids.shape[0]):\n\n            \n            curr_example_id = example_ids[batch_idx]\n            selected_logit_indices = []\n            selected_input_id_indices = []\n            for candidate_logit_idx in range(logits[batch_idx].shape[0]):\n                if candidate_logit_idx + 1 >= input_ids[batch_idx].shape[0]:\n                    continue\n                \n                if input_ids[batch_idx][candidate_logit_idx] == model.config.pad_token_id:\n                    continue\n\n                if input_ids[batch_idx][candidate_logit_idx + 1] == model.config.pad_token_id:\n                    continue\n                    \n                if mask[batch_idx][candidate_logit_idx] == 0:\n                    continue\n                \n                selected_logit_indices.append(candidate_logit_idx)\n                selected_input_id_indices.append(candidate_logit_idx + 1)\n\n            \n            \n            \n            selected_logits = logits[batch_idx][selected_logit_indices].unsqueeze(0)\n            selected_input_ids = input_ids[batch_idx][selected_input_id_indices].unsqueeze(0)\n            \n\n            temp_scores = implied_scorer.get_ev_of_temp(selected_logits[0], selected_input_ids[0])\n            \n            \n            disc, miu, sigma = get_sampling_discrepancy_analytic(selected_logits,selected_logits, selected_input_ids)\n            \n            \n            \n            logprobs = torch.log_softmax(selected_logits, dim=-1)\n            actual_logprobs = logprobs[0][torch.arange(logprobs[0].size(0)), selected_input_ids[0]].cpu().numpy()\n            \n            percentiles = np.percentile(logprobs[:, :, unique_random_token_indices], [10,20,30,40,50,60,70,80,90])\n            \n            \n            actual_percentiles = np.percentile(actual_logprobs, [10,20,30,40,50,60,70,80,90])\n            temp_score_percentiles = np.percentile(temp_scores, [10,20,30,40,50,60,70,80,90])\n\n            \n            id_to_heuristics[curr_example_id] = {\n                \"analytical_disc\": disc,\n                \"negative_entropy\": miu,\n                \"sigma\": sigma,\n                \"temp_score_mean\": np.mean(temp_scores),\n                \"temp_score_std\": np.std(temp_scores),\n                \"logprobs_mean\": np.mean(actual_logprobs),\n                \"logprobs_std\": np.std(actual_logprobs),\n                \"num_tokens\": selected_logits[0].shape[0],\n                \"logprob_percentiles\": percentiles.tolist(),\n                \"actual_logprob_percentiles\": actual_percentiles.tolist(),\n                \"tempscore_percentiles\": temp_score_percentiles.tolist()\n            }\n    \n    return id_to_heuristics\n\nif mistral_heuristic_cache is None:\n    test_quantiles = get_heuristics(test)\n    \nelse:\n    test_quantiles = mistral_heuristic_cache[\"test\"]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\ndel implied_scorer\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unpack_dict(data_dict):\n    # Initialize an empty list for the flattened percentiles\n    flattened_percentiles = []\n\n    # Iterate through each sublist in the nested 'logprob_percentiles' and extend the flat list\n    for percentile in data_dict[\"logprob_percentiles\"]:\n        flattened_percentiles.append(percentile)\n        \n    for percentile in data_dict[\"tempscore_percentiles\"]:\n        flattened_percentiles.append(percentile)\n        \n    for percentile in data_dict[\"actual_logprob_percentiles\"]:\n        flattened_percentiles.append(percentile)\n\n    \n    flattened_data = [\n        data_dict[\"analytical_disc\"],\n        data_dict[\"negative_entropy\"],\n        data_dict[\"sigma\"],\n        data_dict[\"temp_score_mean\"],\n        data_dict[\"temp_score_std\"],\n        data_dict[\"logprobs_mean\"],\n        data_dict[\"logprobs_std\"],\n        data_dict[\"num_tokens\"],\n    ] + flattened_percentiles\n\n    return flattened_data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = daigt2_train['label'].values\ny_val = daigt2_val['label'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract Feature","metadata":{}},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 30522","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC(), normalizers.StripAccents(), normalizers.Strip()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\ndataset = Dataset.from_pandas(test[['text']])\ndef train_corp_iter(): \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\ncustom_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(custom_tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(daigt2_train['text'].tolist()):\n    tokenized_texts_train.append(custom_tokenizer.tokenize(text))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dummy(text):\n    return text\nvectorizer = TfidfVectorizer(\n    ngram_range=(3, 7), lowercase=False, sublinear_tf=True, analyzer = 'word', max_df=0.99, max_features=5000000,\n    tokenizer = dummy,\n    preprocessor = dummy,\n    token_pattern = None,\n)\n\nvectorizer.fit(tokenized_texts_test)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\n\ndel vectorizer\ngc.collect()\nlibc.malloc_trim(0)\n\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 7), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n    analyzer = 'word',\n    tokenizer = dummy,\n    preprocessor = dummy,\n    token_pattern = None\n)\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler(with_mean=False)\nscalar.fit(tf_train)\nfeature_stds = np.sqrt(scalar.var_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB(alpha=0.02)\n#     clf2 = MultinomialNB(alpha=0.01)\nsgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \np6={'n_iter': int(2500 * GBOOST_ITERATION_FRACTION),\n    'verbose': -1,\n    'objective': 'cross_entropy',\n    'learning_rate': 0.003, \n    'colsample_bytree': 0.8,\n    'colsample_bynode': 0.4}\n\nlgb=LGBMClassifier(**p6)\ncat=CatBoostClassifier(iterations=int(350*GBOOST_ITERATION_FRACTION),\n   verbose=0,\n   l2_leaf_reg=6.6591278779517808,\n   learning_rate=0.01,\n   subsample = 0.4,\n   allow_const_label=True,loss_function = 'CrossEntropy')\n\n\nprint(\"fitting naive bayes\")\nclf.fit(tf_train, y_train)\nprint(\"done fitting naive bayes\")\nprint(\"fitting sgd\")\nsgd_model.fit(tf_train, y_train)\nprint(\"done fitting sgd\")\n\nprint(\"predicting with SGD and MNB\")\nclf_preds = clf.predict_proba(tf_test)[:,1]\nsgd_preds = sgd_model.predict_proba(tf_test)[:,1]\n\nprint(\"done predicting with SGD and MNB\")\n\n# kept_feature_indices = []\n# for i in range(len(sgd_model.coef_[0])):\n#     if abs(sgd_model.coef_[0][i]* feature_stds[i]) > .001:\n#         kept_feature_indices.append(i)\n\n# print(f\"kept {len(kept_feature_indices)} out of {len(sgd_model.coef_[0])} features\")\nfeature_importances = np.abs(sgd_model.coef_[0] * feature_stds)\n\n# Get the indices of the top 3K values\nkept_feature_indices = np.argsort(feature_importances)[-6000:]\n\n# now replace the tf_train and tf_test with only the kept features\ntf_train_trimmed = tf_train\ntf_test_trimmed = tf_test\n\n# print(\"fitting catboost\")\n# cat.fit(tf_train_trimmed, y_train)\n# print(\"done fitting catboost\")\nprint(\"fitting lgbm\")\nlgb.fit(tf_train_trimmed, y_train)\ncat.fit(tf_train_trimmed, y_train)\nprint(\"done fitting lgbm\")\ngc.collect()\n\nlgb_preds = .666 * lgb.predict_proba(tf_test_trimmed)[:,1] + .333 * cat.predict_proba(tf_test_trimmed)[:,1]\n# cat_preds = cat.predict_proba(tf_test_trimmed)[:,1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id_to_prob_pred = {}\nfor example_id, sgd_pred, mnb_pred, lgb_pred in zip(test[\"id\"].values, sgd_preds, clf_preds, lgb_preds):\n    test_id_to_prob_pred[example_id] = [sgd_pred, mnb_pred, lgb_pred]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_prompt_ids = test[\"prompt_id\"].unique()\nnum_prompt_ids = len(unique_prompt_ids)\n\n# make a map from prompt id to range(num_prompt_ids)\nprompt_id_to_idx = {}\nfor i, prompt_id in enumerate(unique_prompt_ids):\n    prompt_id_to_idx[prompt_id] = i","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id_to_pseudo_label = {}\ntest_id_to_heuristic_features = {}\nfor test_row_idx, test_row in tqdm(test.iterrows()):\n    test_id = test_row[\"id\"]\n    test_id_to_pseudo_label[test_id] = np.dot(test_id_to_prob_pred[test_id], [0.33, 0.09, 0.57])\n    test_id_to_heuristic_features[test_id] = unpack_dict(test_quantiles[test_id])# + [mean_deberta_preds[test_id]]\n    \n    prompt_id_features = num_prompt_ids * [0]\n    prompt_id_features[prompt_id_to_idx[test_row[\"prompt_id\"]]] = 1\n    test_id_to_heuristic_features[test_id].extend(prompt_id_features)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_predict, KFold\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n\nheuristic_feature_matrix = []\npseudo_labels = []\nfor test_row_idx, test_row in tqdm(test.iterrows()):\n    test_id = test_row[\"id\"]\n    pseudo_labels.append(test_id_to_pseudo_label[test_id])\n    heuristic_feature_matrix.append(test_id_to_heuristic_features[test_id])\n    \nheuristic_feature_matrix = np.array(heuristic_feature_matrix)\npseudo_labels = safe_logodds(pseudo_labels)\n\nmodel2 = GradientBoostingRegressor()\nmodel3 = RandomForestRegressor()\nmodel4 = LinearRegression()\n\n# Create Soft Voting Regressor\npseudo_regressor = VotingRegressor(\n    [\n        ('gbr', model2), \n#         ('rfr', model3),\n        ('lr', model4)\n    ],\n    weights=[1,1]  # equal weights for demonstration\n)\n\nprint(\"cross val predicting\")\n# pred_from_heuristics = cross_val_predict(pseudo_regressor, heuristic_feature_matrix, pseudo_labels, cv=KFold(n_splits=30, shuffle=True, random_state=1))\npseudo_regressor.fit(heuristic_feature_matrix, pseudo_labels)\npred_from_heuristics = pseudo_regressor.predict(heuristic_feature_matrix)\nprint(\"done cross val predicting\")\n\nfinal_preds = []\nfor pred_from_heuristic, pseudo_label in zip(pred_from_heuristics, pseudo_labels):\n    final_preds.append((FOLLOWER_MODEL_WEIGHT * pred_from_heuristic + pseudo_label) / (1 + FOLLOWER_MODEL_WEIGHT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = np.array(final_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npseudo_regressor.fit(predict_model_features, final_preds)\npred_from_model_features = pseudo_regressor.predict(predict_model_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = torch.sigmoid(torch.as_tensor(.8 * final_preds + .2 * pred_from_model_features)).numpy() * .6 + ertugrul_deberta_predictions * .4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame({'id':test[\"id\"].values[:len(final_preds)],'generated':final_preds})\nsub.to_csv('submission.csv', index=False)\nsub.head(30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}